{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### University of Virginia\n",
    "### DS 5110: Big Data Systems\n",
    "\n",
    "### Lab: Supervised Learning\n",
    "### Last updated: March 2, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "This project has two parts:\n",
    "- Part I: Classification - build and apply a logistic regression model on the Wisconsin Breast Cancer dataset.\n",
    "- Part II: Regression - build and apply a linear regression model on the California Housing dataset.\n",
    "\n",
    "**Total Possible Points: 10**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part I: Classification (5 POINTS)\n",
    "\n",
    "Here are the specifications and grading breakdown:\n",
    "\n",
    "- the target variable is `diagnosis`\n",
    "- use `f1`, `f2` as predictors (1 PT)\n",
    "- split data into 60% training set, 40% test set \n",
    "- standardize the predictors (1 PT)\n",
    "- use seed=314 whenever a seed is needed\n",
    "- fit a Logistic Regression model with an intercept (1 PT)\n",
    "- compute and show the area under the ROC curve for the test set (2 PTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/20 15:32:19 WARN Utils: Your hostname, Beaus-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.4.32 instead (on interface en0)\n",
      "23/09/20 15:32:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/20 15:32:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "DATA_FILEPATH = 'wisc_breast_cancer_w_fields.csv'\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Wisc BRCA\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/20 15:32:28 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-----+-----+-----+------+-------+-------+------+-------+------+-------+------+------+-----+-----+--------+-------+-------+-------+-------+--------+-----+-----+-----+------+------+------+------+------+------+-------+\n",
      "|    id|diagnosis|   f1|   f2|   f3|    f4|     f5|     f6|    f7|     f8|    f9|    f10|   f11|   f12|  f13|  f14|     f15|    f16|    f17|    f18|    f19|     f20|  f21|  f22|  f23|   f24|   f25|   f26|   f27|   f28|   f29|    f30|\n",
      "+------+---------+-----+-----+-----+------+-------+-------+------+-------+------+-------+------+------+-----+-----+--------+-------+-------+-------+-------+--------+-----+-----+-----+------+------+------+------+------+------+-------+\n",
      "|842302|        M|17.99|10.38|122.8|1001.0| 0.1184| 0.2776|0.3001| 0.1471|0.2419|0.07871| 1.095|0.9053|8.589|153.4|0.006399|0.04904|0.05373|0.01587|0.03003|0.006193|25.38|17.33|184.6|2019.0|0.1622|0.6656|0.7119|0.2654|0.4601| 0.1189|\n",
      "|842517|        M|20.57|17.77|132.9|1326.0|0.08474|0.07864|0.0869|0.07017|0.1812|0.05667|0.5435|0.7339|3.398|74.08|0.005225|0.01308| 0.0186| 0.0134|0.01389|0.003532|24.99|23.41|158.8|1956.0|0.1238|0.1866|0.2416| 0.186| 0.275|0.08902|\n",
      "+------+---------+-----+-----+-----+------+-------+-------+------+-------+------+-------+------+------+-----+-----+--------+-------+-------+-------+-------+--------+-----+-----+-----+------+------+------+------+------+------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithSGD, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# read data into dataframe\n",
    "training = spark.read.csv(DATA_FILEPATH,  inferSchema=True, header = True)\n",
    "training.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = training.randomSplit([.6,.4], 314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "stringIndexer = StringIndexer(inputCols = ['diagnosis'], outputCols=['diagnosis_numeric'], handleInvalid = 'skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# inputCols take a list of column names\n",
    "# outputCol is arbitrary name of new column; generally called features\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"f1\", \"f2\"],\n",
    "                            outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=8670, diagnosis='M', f1=15.46, f2=19.48, f3=101.7, f4=748.9, f5=0.1092, f6=0.1223, f7=0.1466, f8=0.08087, f9=0.1931, f10=0.05796, f11=0.4743, f12=0.7859, f13=3.094, f14=48.31, f15=0.00624, f16=0.01484, f17=0.02813, f18=0.01093, f19=0.01397, f20=0.002461, f21=19.26, f22=26.0, f23=124.9, f24=1156.0, f25=0.1546, f26=0.2394, f27=0.3791, f28=0.1514, f29=0.2837, f30=0.08019),\n",
       " Row(id=8913, diagnosis='B', f1=12.89, f2=13.12, f3=81.89, f4=515.9, f5=0.06955, f6=0.03729, f7=0.0226, f8=0.01171, f9=0.1337, f10=0.05581, f11=0.1532, f12=0.469, f13=1.115, f14=12.68, f15=0.004731, f16=0.01345, f17=0.01652, f18=0.005905, f19=0.01619, f20=0.002081, f21=13.62, f22=15.54, f23=87.4, f24=577.0, f25=0.09616, f26=0.1147, f27=0.1186, f28=0.05366, f29=0.2309, f30=0.06915),\n",
       " Row(id=8915, diagnosis='B', f1=14.96, f2=19.1, f3=97.03, f4=687.3, f5=0.08992, f6=0.09823, f7=0.0594, f8=0.04819, f9=0.1879, f10=0.05852, f11=0.2877, f12=0.948, f13=2.171, f14=24.87, f15=0.005332, f16=0.02115, f17=0.01536, f18=0.01187, f19=0.01522, f20=0.002815, f21=16.25, f22=26.19, f23=109.1, f24=809.8, f25=0.1313, f26=0.303, f27=0.1804, f28=0.1489, f29=0.2962, f30=0.08472),\n",
       " Row(id=85715, diagnosis='M', f1=13.17, f2=18.66, f3=85.98, f4=534.6, f5=0.1158, f6=0.1231, f7=0.1226, f8=0.0734, f9=0.2128, f10=0.06777, f11=0.2871, f12=0.8937, f13=1.897, f14=24.25, f15=0.006532, f16=0.02336, f17=0.02905, f18=0.01215, f19=0.01743, f20=0.003643, f21=15.67, f22=27.95, f23=102.8, f24=759.4, f25=0.1786, f26=0.4166, f27=0.5006, f28=0.2088, f29=0.39, f30=0.1179),\n",
       " Row(id=86211, diagnosis='B', f1=12.18, f2=17.84, f3=77.79, f4=451.1, f5=0.1045, f6=0.07057, f7=0.0249, f8=0.02941, f9=0.19, f10=0.06635, f11=0.3661, f12=1.511, f13=2.41, f14=24.44, f15=0.005433, f16=0.01179, f17=0.01131, f18=0.01519, f19=0.0222, f20=0.003408, f21=12.83, f22=20.92, f23=82.14, f24=495.2, f25=0.114, f26=0.09358, f27=0.0498, f28=0.05882, f29=0.2227, f30=0.07376)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/20 15:32:36 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "lr = LogisticRegression(labelCol='diagnosis_numeric',\n",
    "                        featuresCol='scaledFeatures',\n",
    "                        maxIter=10, \n",
    "                        regParam=0.3, \n",
    "                        elasticNetParam=0.8)\n",
    "\n",
    "pipeline = Pipeline(stages = [stringIndexer, assembler, scaler, lr])\n",
    "# Fit the model\n",
    "lrModel = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC Curve: 0.6959459459459459\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "predDF = lrModel.transform(test)\n",
    "\n",
    "# set up evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",\n",
    "                                          labelCol=\"diagnosis_numeric\",\n",
    "                                          metricName=\"areaUnderROC\")\n",
    "\n",
    "# pass to evaluator the DF with predictions, labels\n",
    "auc = evaluator.evaluate(predDF)\n",
    "\n",
    "print(\"Area under ROC Curve:\", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter code and solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part II: Regression (5 POINTS)\n",
    "\n",
    "In this project, you will work with the California Home Price dataset to train a regression model and predict median home prices. Here are the specifications and grading breakdown:\n",
    "\n",
    "- Scale the response variable median_house_value, dividing by 100000 (1 PT)\n",
    "\n",
    "- Split data into train set (80%), test set (20%) using seed=314 (1 PT)\n",
    "\n",
    "- Add new predictor: `rooms_per_household`\n",
    "\n",
    "- In the training set, select all of these features and standardize them: (1 PT)\n",
    "\n",
    "feats = [\"total_bedrooms\", \n",
    "         \"population\", \n",
    "         \"households\", \n",
    "         \"median_income\", \n",
    "         \"rooms_per_household\"]\n",
    "\n",
    "- Fit a linear regression model on the training set with these parameters:\n",
    "\n",
    "  - maxIter=10\n",
    "  - regParam=0.3\n",
    "  - elasticNetParam=0.8  \n",
    "\n",
    "\n",
    "- Compute the MSE on the test set (2 PTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILEPATH2 = 'cal_housing_data_preproc_w_header.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "|median_house_value|median_income|housing_median_age|total_rooms|total_bedrooms|population|households|latitude|longitude|\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "|          452600.0|       8.3252|              41.0|      880.0|         129.0|     322.0|     126.0|   37.88|  -122.23|\n",
      "|          358500.0|       8.3014|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|   37.86|  -122.22|\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(DATA_FILEPATH2,  inferSchema=True, header = True)\n",
    "data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "|median_house_value|    median_income|housing_median_age|total_rooms|total_bedrooms|population|households|latitude|longitude|\n",
      "+------------------+-----------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "|             4.526|           8.3252|              41.0|      880.0|         129.0|     322.0|     126.0|   37.88|  -122.23|\n",
      "|             3.585|           8.3014|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|   37.86|  -122.22|\n",
      "|             3.521|7.257399999999999|              52.0|     1467.0|         190.0|     496.0|     177.0|   37.85|  -122.24|\n",
      "|             3.413|           5.6431|              52.0|     1274.0|         235.0|     558.0|     219.0|   37.85|  -122.25|\n",
      "|             3.422|           3.8462|              52.0|     1627.0|         280.0|     565.0|     259.0|   37.85|  -122.25|\n",
      "+------------------+-----------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumn('median_house_value', data['median_house_value']/100000)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+\n",
      "|median_house_value|    median_income|housing_median_age|total_rooms|total_bedrooms|population|households|latitude|longitude|rooms_per_household|\n",
      "+------------------+-----------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+\n",
      "|             4.526|           8.3252|              41.0|      880.0|         129.0|     322.0|     126.0|   37.88|  -122.23|  6.984126984126984|\n",
      "|             3.585|           8.3014|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|   37.86|  -122.22|  6.238137082601054|\n",
      "|             3.521|7.257399999999999|              52.0|     1467.0|         190.0|     496.0|     177.0|   37.85|  -122.24|  8.288135593220339|\n",
      "|             3.413|           5.6431|              52.0|     1274.0|         235.0|     558.0|     219.0|   37.85|  -122.25| 5.8173515981735155|\n",
      "|             3.422|           3.8462|              52.0|     1627.0|         280.0|     565.0|     259.0|   37.85|  -122.25|  6.281853281853282|\n",
      "+------------------+-----------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumn('rooms_per_household', data['total_rooms']/data['households'])\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data.randomSplit([.8,.2], 314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"total_bedrooms\", \"population\", \"households\", \"median_income\", \"rooms_per_household\"],\n",
    "                            outputCol=\"features\")\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "lr = LinearRegression(labelCol='median_house_value',\n",
    "                        featuresCol='scaledFeatures',\n",
    "                        maxIter=10, \n",
    "                        regParam=0.3, \n",
    "                        elasticNetParam=0.8)\n",
    "\n",
    "pipeline = Pipeline(stages = [assembler, scaler, lr])\n",
    "# Fit the model\n",
    "lrModel = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE is 0.7551749809615468\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "predDF = lrModel.transform(test)\n",
    "\n",
    "regressionEvaluator = RegressionEvaluator(\n",
    "    predictionCol = 'prediction',\n",
    "    labelCol = 'median_house_value',\n",
    "    metricName= 'mse')\n",
    "mse = regressionEvaluator.evaluate(predDF)\n",
    "print(f\"MSE is {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter code and solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
